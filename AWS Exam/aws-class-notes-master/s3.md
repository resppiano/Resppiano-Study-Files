# S3 (Simple Storage Service)

## S3 101

among the first services made by AWS. It's a big deal. Important in all the exams.

In essentials: it stores objects of anysize that you can retrieve anywhere on the web. Any files at all.

It's *not* block baced storage

files from 0-5 TB.

Ulimited storage, you pay by the gig.

files are in "buckets" which are essentially top level folders. Buckets can have folders in it. Bucket is in a universal namespace so it has to be unique.

s3-{region}.amazonaws.com/{bucketname} is always url

upload requests give you an http 200 response

### Data consistency model

read after write consistencey for new objects

eventual consistencey for overwrites and deletes. They make take a little while to propogate (like a minute)

### s3 is key-value

an object comprises: A key (name) + data (sequence of bytes) + Version ID + metadata (upload date, tags, etc.)

subresources inside s3 include Access control lists (essentially permissions on files), torrents (beyond scope of course/exam)

### basics

built for 99.99 availability. guarantee 3 9s, and 11 9s for durability. ^very^ unlikely to lose a file in s3

tiered storage is available.

lifecycle management (move files after a certain amount of time)

versioning on files

encrypted files

access control with lists and policies that are specific to buckets.

### storage classes

s3 standard is 4 9s available, 11 9s durability, and designed to handled the loss of 2 facilities. Pretty much everyone uses this

S3 - IA is infrequently accessed, but when you need it you need it fast. Lower fee but you pay per retrieval

S3- One Zone - IA is even cheaper than IA since it's only in 1 zone.

Glacier is the cheapest, it's ussed for archiving. Expidited retrieval is expensive but you get data in like 10 minutes, standard retrieval is 3-5 hours, Bulk is like 12 hours.

THere's a table of storage tiers/classes for these different types that lays out the differences

Essentially there's a sliding scale of speed/cost standard > IA > IA 1 zone > glacier

charges are for: Storage per gig, per request, management pricing (like tags), transers, transfer accelaration (for global data transfers using cloudfront)

### exam tips

remember s3 is object based

file size is 0 to 5TB

storage is unlimited

files are store in bucket

bucket names are universally namespaced

url is s3-{region}.amazonaws.com/{bucket name}

consistency model is read after write for new objects and eventually consistent for updates and deletes

tiers are:

* standard
* IA
* one zone IA
* glacier

and know what the differences are

an s3 object has a key (name) value (data in bytes) version ID, and metadata, and subresrouces are Access control and torrent data

S3 is only for objects, not an OS or a database etc.

read s3 faqs before exam because it comes up a LOT

## Labs

### Create an s3 bucket

S3 buckets are managed at a global level. Console has one big list. Makes sense since the namespace is global

Created a `tyler-romeo-lab` bucket in US-East N. Virginia

went through properties, and added a "tag". Can also set up logging, verisoning and encryption

permissions tab lets you add accounts, public permissions and system permissions.

By default, bucket permissions are private

uploaded some files through the web console to see them in there.

If we did this through the console we'd get an http 200

went into the file uploaded and clicked "make public" so the url works

looked at permissions tab. There are 3 types of permissions: owner, other accounts (there's a list) and public. Can set read or write on all of those.

There's a properties tab where you can set the storage class (cost/availability scale), encryption, Metadata, and Tags

Metadata is headers that get set when you http at the object, which is neat.

Tags can be on individual object. Objects do not inherit the bucket's tags.

on permissions tab you can go throught he access control list, and also set up policies. There's in in browser editor and also a "policy generator" tool in the browser. policy generator is a little out of scope

management tab lets us look at lifecycle, replication, analytics, metrics, and inventory.

Notes on ecryption: can do it client side, or server side with S3 managed keys, with KMS, or with Customer Provided Keys.

### versioning

in properties tab, enable versioning.

Once versioning is enabled it can *never* be disabled, only suspended.

This can increase your storage costs if your files change very often.

uploaded a test file to play with.

made a change and uploaded again, and you can see in s3 that there are both versions.

Can download old file with a link, or could delete the new version which will restore the old version.

Could also delete the object entirely. HOWEVER, deleted files still have the old versions with a "delete" marker. You can remove this marker to restore the object.

remember that every upload makes a new copy of the file, so your storage space usage can balloon. Each version is its own object.

s3 versioning docs are useful to read through to get into the technical details.

An important thing on these docs is MFA delete. That means you can add a hardware token requirement for deleting objects or changing the version control strategy.

#### exam tips

versioning stores every version of an object

it's good for backups

once it's enabled it can't be turned off, only suspended

integrates with lifecycle tools (we'll get into this later)

Has MFA delete option for further protecting your backups

### cross-region replication

created a new bucket in the us west bucket

have to enable versioning for cross region replication to work

can replicate an entire bucket or just a sub folder

pick a destination bucket for replication. Can change the storage class for destinaion (IA is good here probably)

need an IAM role for doing the replication (it can create a new one for you)

replication is only for NEW or CHANGED object, it does not do existing objects

to copy over the contents of an existing bucket, you can use the cli

#### sidebar, setting up the aws cli

already had this installed

making a new IAM group for CLI access

once cli was set up with a new user, ran: `aws s3 ls` to see all the buckets

`aws s3 cp --recursive s3://tyler-romeo-lab s://tyler-romeo-westcoast` to copy files from one bucket to the other

after copying over, deleted a file from main bucket. That delete marker gets replicated over

however, when you delete the delete marker, it does NOT get replicated. Just something to know

updated a file in the main bucket, that update gets replicated to the other region

interesting note the old versions of my objects in the original bucklet did not get `cp`d over.

another weird thing, if you revert to an old version by deleting from a source bucket, that revert doesn't get replcated. Have to do that manually.

Looks like cross region updates are best when there's not a lot of deleting going on in the source bucket

#### Exam tips

both source and destination must have versioning turned on

can't replicate into the same bucket with this tool

files in existing bucket don't get replicated, it just applies updates to both buckets.

Can't replicated to multiple destinations, and can't daisy chain from bucket to bucket (right now)

delete markers are replicated (since they are objects) but *deletions* of objects are not replicated. It's a weird distinction

Understand the high level implications of cross region replication.

### Lifecycle Management, S3-IA & Glacier

Use case includes needing to store files for a long time, but not get to them very often

Made a new s3 bucket with versioning turned on

went into settings under management tab to look at lifecycle rules

lifecycle rules define how s3 manages objects, it's a way to automatically manage tiered storage, auto-expire objects, etc.

can apply lifecycle rules to specific filters/tags if you want

added rule to transition objects to standard-IA after 30 days, and another for moving to Glacier after 60 days.

"previous versions" can have their own separate rules. That's how it deals with old versions of objects when versinoing is turned on

set up rules to "expire" objects after 425 days

Not super complex, but important

#### Exam tips

Can be used with versioning

Can be used with bot cuyrrent and previous versions

Can transition to standard IA, can archive to Glacier, or can permenantly deleted after set intervals

Will probably get csenario questions about minimizing storage costs, this is how you would accomplish that.

### CloudFront CDN

a CDN is a system of geographically distributed services that serve data.

I mean you know what a CDN is, Tyler.

Cloudfront terms:

* edge location - the actual location where the content is cached. There are lots of these, different from a region/AZ
* Origin - the location of the files the CDN distributes. Can be S3 bucket, EC2 instances, ELB or Route 53. (probabyl S3 or EC2)
* Distribution the name of the CDN, comprises a list of edge locations.
* Web distribution - used for websites
* RTMP - used for media streaming (particularly flash)

Requests go to edge locations first, and check if the content is cached at that location.

If the edge location does not have it cached, it goes and gets it from the source and adds it to the cache with a TTL.

In addition to AWS service, cloudfront can actually work with non-AWS origin servers

#### Exam tips

Know what an edge location is and it's distinction from an AZ/Region

Origin is the source of the files the CDN distributes

Distribution is the name given to the CDN, which is a colleciton of edge lcoations

Distributions can be Web Distributions or RTMP

Edge locations are *not* read only, you can write too and that change will be forwarded to the origin

Objects are cached for a TTL,

Can clear an object from the cache manually, however there is a charge for this.

### Cloudfront lab

have an existing bucket with an image in it

creating a new cloudfront with "Web" distribution

autocompleted with s3 buckets we had

can set an originpath to use a subfolder in an s3 bucket if you want. We're using the root by leaving it blank

OriginId lets you have multiple origins in 1 distribution. Left as default.

Restrict bucket access prevents requests from going to s3 url directly, has to go through cloudfront. When turning this on, we had to creat a new Identiry with read permission on the bucket. Form did this for us

path pattern is a regex that lets you select the right origin.

can set http/https policies, and allowed http methods.

can set allowed http methods, which lets you enable post put etc and write to edge locations.

TTLs are listed this form, the unit is Seconds. Looks like you can't change it here?

Restrict user access lets you configure signed urls or signed cookies to get access. This is how you serve private content through cloudfront

Distribution settings lets you set which edge locations are used.

Can use an AWS WAF firewall to secure cloudfront (not in exam yet)

Can set up your ssl certs or use default

Can set up alternate domains (going to go over this more in the route53 lession). If you set your own domain name you'd want to upload your own cert here.

Default root object is what object should be returned when accessing the root url (used for setting an index.html primarily)

Logging settings in here too.

Created the distribution, it has to get created which takes 10 minutes or so.

going into settings to look at options including origins (where data is sourced), behaviors (routing to origins), error pages, restrictions (geography based) with a whitelist or a blacklist. Can't whitelist and blacklist at the same time (duh), and invalidations, which removes objects from edge locations before the TTL is due (costs money)

Deleted distribution after the lab with the disable button.

### S3 security and encryption

buckets are private by default, so the files can't be accessed by the public.

You can apply policies to the entire bucket ro control access

Or you can use access control lists to drill down the the individual object level.

Buckets can also be configures to create access logs for all requests. It adds those logs to either that bucket, or a different bucket.

Encryption: there are 4 methods you'll need to know

In transit: sending data to/from bucket. Done with TLS/SSL

At rest has 4 methods:

Server side encryption

* S3 managed keys. data and keys are encrypted and rotated. Uses aes-256. All automatic, just toggle it on. Most common
* KMS managed keys. Use keys that are managed by AWS's KMS service. Uses the KMS envelope key, gets a generated audit trail. Called SSE-KMS
* SSE-C, customer provided keys. You provide the keys for s3 to use to encrypt/decrypt.

Client side encyption is when you encryt the data before you ever send to to S3. That means you'll have to decrypt it too.

### Storage gateway

NB this is a big exam topic.

No labs

Connects on on prem software appliance to cloud storage

virtual client in your data center replicates your data up to s3 or glacier.

you can download Starage Gateway as a VM and install it on your data cluster

Activate it with your aws credentials, then go into console to activate it

Storage gateway types:

* File Gateway (NFS) stores flat files in S3
* Volumes Gateway (iSCSI) block based storage. For OSes or applications
* VG has stored volumes and cached volumes. Stored volumes keeps them all in AWS, and cached volumes keeps old versions in s3 while your latest version lives on-site
* Tape Gateway (VTL). Create virtual tape backups and send them to s3

These options used to have different names, but they are similar enopugh to figure out

#### File gateway

This is new so it may not show up in the exam

files are stored as objects in S3 through an NFS. Once they are in s3 you can do S3 stufdf to them. Lifecycle etc.

Coudl do this in a VPC too if you want, doesn't *have* to be on prem, even though that's the most common use case

#### Volume gateway

Backups for your disk volumes. Block based storage.

OSes, applications, databases, etc.

Think of it like a virtual hard disk

Gets backed up asyncronously, and backed up to AWS EBS (Elastic block storage)

incremental snapshots (so only changes are backed up) to minimize storage costs

Stored volumes have a backup device locally that gets backed up. So you take your backups to that disk, and then AWS backs it up. Size is 1 GB to 16 TB

Virtual hard disks are created by the Storage gateway, but they are on your hardware. Then these virtual volumes are backed up to s3.

Cached Volumes use s3 as the *primary* storage. A complete copy of the data isn't on prem, only the most recent data is. Up to 32 TB is allowed with this version.

Tape gateway uses a virtual tape library interface. It's for if you already have infrastructure for doing tape backups and you want to turn your physical tapes into virtual dates and they will go into s3.

#### Exam tips

File gateway if for flat files, they get backed up directly to s3

Volume gateway is for block based storage

VG can be stored volumes or cached volumes

stored volumes back up to a local virtual hard disk and gets backed up to s3

cached volumes stores only the latest backup on local, all the rest is in s3

tape library (VTL) is for existing tape infrastructure like NetBackup, Veeam etc. Uses virtual tape.

know the 4 gateways and their use cases in the exam

### Snowball

before there was snowball the service was called "import/export disk". It was a portable device that you coudl write data to and then send back to AWS for them to put in teh cloud.

It's sneakernet for AWS

import/export disk was tought to manage becuase they were getting all kinds of disks.

Snowball, snowball edge, and snowmobile are three new types of device.

Snowballs are big and heavy. Cheaper than sending data over the internet.

snowballs are encrypted and safe, you get tracking info and can follow the chain of custody.

Snowballs are wiped after they are transferred into AWS

Snowball edge is bigger, and had onboard storage *AND* compute capabilities.

edge is basically a little AWS datacenter.

You can run lambda functions on an edge.

Use case is somewhere without internet, on an airplane etc.

Snowmobile is a semi truck. It's for Petabytes or Exobytes of data. 100PB per snowmobile.

#### Exam tips

know what snowball is

know that it was called import/export, and that import export involved sneding in your own disks

Snowball can import to s3 and export to s3.

To get data out of glacier you'll have to export to s3 first then get a snowball sent.

#### Snowball lab

huh?

dude has got a snowball in his office for this video

he created a job to get them to deliver him a snowball. It's in the "Delivered to you" state

He is going to put a single text file on it

when ordering you set up an s3 bucket for it, set up security, and set up sns for notifications

You have to download the client to communicate w/ snowball, theres mac, windows and linux

Video of the snowball. It's comically large. Like 4 X 4 X 2

He's opening it up on camera, it's pretty much an unboxing video.

There's a kindle embedded in it as a monitor, kinda neat.

The cables are all embedded in the device. Power goes to wall, ethernet cable goes into network. You have to buy your own kettle plug adapter.

It's real loud when it turns on

to use CLI, use the snowbal dashboard to get the credentials.

start it with the cli. Need manifest file and access code

syntax is similar to s3, `snowball cp <source> <destination>

use the s3 bucket that snowball is going to end up in as the destination

### S3 transfer accelleration

This is relatively new

It uses the edge network to make your uploads to s3 faster

you get a new, distinct accelerate url to upload to

to enabled it, you go through the s3 console

it's in the properties tab. You can enable it and it's on.

You get the new endpoint to PUT to.

Yoou get hit with an additional fee when you have this on.

There's a link that detects upload speed of direct upload speed vs accellerated upload speed.

It actually makes some places faster and some places slower.

Probably not in the exam, but good to be aware of.

### lab - create/host a static website

side note, if you want route53 to work with s3 for a regular domain, you'll need the domain name to match the bucket name.

enable static website hosting in properties

KNOW URL FORMAT

`http://<bucket-name>.s3-website-<region>.amazonaws.com`

set an index document and an error document.

Can also set up some redirect rules

created a couple of basic html files to upload.

Upload the files just like regular s3 files, make sure to set the access to public.

It's a website so you probably want standard storage class.

click the link in your properies and you can see the website!

type a bad url and you'll see the error page

it bears repeating that the content can't be dynamic. No php or whatever.

Scales *infinitely*. The hosting will always stay up. S3 autoscales the website since it's static content.

exam scenario will be something like a preview site for a highly anticipated movie

exam will also have the url format.

### Summary

There's a summary video at `https://www.udemy.com/aws-certified-solutions-architect-associate/learn/v4/t/lecture/4325548?start=0`

notes: One zone used to be called RRS, remember that.

RRS only has 4 9s of durability.

Can use signed urls to reduce hotlinking to images